{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b31d6d-17c4-46c4-b994-41cfab5b189d",
   "metadata": {},
   "source": [
    "### **Using Surprise Library to train a collaborative filtering model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400ec094-2126-4276-86d7-7eb010b84a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from surprise import (\n",
    "    Reader, Dataset, accuracy,\n",
    "    NormalPredictor, KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline,\n",
    "    SVD, SVDpp, BaselineOnly, NMF, SlopeOne, CoClustering\n",
    ")\n",
    "from surprise.accuracy import rmse\n",
    "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a4be8cb-4994-4bf0-ac46-86d7661c4459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "books_df = pd.read_csv('books_df_processed.csv')\n",
    "users_df = pd.read_csv('users_df_processed.csv')\n",
    "rating_df = pd.read_csv('rating_df_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae535d4b-a894-49a5-8f05-e68f1487472c",
   "metadata": {},
   "source": [
    "### **Scenario 1: Randomly sample 50,000 books as kernel keeps on dying**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e91ce9-84a8-4f85-8bf2-6d55765fe693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "books_sampled = books_df.sample(n=50000, random_state=42)\n",
    "\n",
    "books_sampled.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8159e96-6c25-49c8-b8a5-be9c97057939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct books: 27314\n",
      "Number of distinct users: 23682\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(books_sampled, rating_df, on='ISBN', how='inner')\n",
    "\n",
    "df = df[~(df['Book-Rating'] == 0)] # removing book rating that are marked as 0\n",
    "\n",
    "distinct_books = df['ISBN'].nunique()\n",
    "print(f\"Number of distinct books: {distinct_books}\")\n",
    "distinct_users = df['User-ID'].nunique()\n",
    "print(f\"Number of distinct users: {distinct_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f83e3e5-c051-4671-8c3c-e3d4e3b29088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISBN                   67644\n",
       "Book-Title             67644\n",
       "Book-Author            67644\n",
       "Year-Of-Publication    67644\n",
       "Publisher              67644\n",
       "Image-URL-S            67644\n",
       "Image-URL-M            67644\n",
       "Image-URL-L            67644\n",
       "User-ID                67644\n",
       "Book-Rating            67644\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f7045c-c818-42c4-b760-ab31dc16d15a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 10))\n",
    "data_full = Dataset.load_from_df(df[['User-ID', 'ISBN', 'Book-Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f5e674d-1ce3-466a-ba32-3945076b399b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "    test_rmse  fit_time  test_time        Algorithm\n",
      "0    1.698206  0.874003   0.147972              SVD\n",
      "1    1.698957  3.956379   0.683426            SVDpp\n",
      "2    1.918814  5.011092   0.413685         SlopeOne\n",
      "3    2.455313  3.383032   0.164723              NMF\n",
      "4    2.493555  0.063126   0.080993  NormalPredictor\n",
      "5    1.771991  2.499006   0.428797      KNNBaseline\n",
      "6    1.901145  2.771136   0.627343         KNNBasic\n",
      "7    1.876307  2.593918   0.354147     KNNWithMeans\n",
      "8    1.876055  3.261279   0.394614    KNNWithZScore\n",
      "9    1.707776  0.277886   0.203773     BaselineOnly\n",
      "10   1.933295  2.805353   0.079597     CoClustering\n"
     ]
    }
   ],
   "source": [
    "benchmark = []\n",
    "\n",
    "# Iterate over different algorithms, do cross-validation, and store the RMSE and compare them\n",
    "for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "    results = cross_validate(algorithm, data_full, measures=['RMSE'], cv=3, verbose=False)\n",
    "\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    algo_name = str(algorithm).split(' ')[0].split('.')[-1]\n",
    "    tmp = pd.concat([tmp, pd.Series({'Algorithm': algo_name})])\n",
    "    benchmark.append(tmp)\n",
    "\n",
    "benchmark_df = pd.DataFrame(benchmark)\n",
    "print(benchmark_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f7eb7-c6f0-4e45-9541-77317e2b483e",
   "metadata": {},
   "source": [
    "### **Scenaior 2: creating another dataframe where each book has at least 2 ratings and each user has 20 ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "074a76e2-949d-4381-9e22-bee0f3e219cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct books: 43968\n",
      "Number of distinct users: 6511\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min_book_ratings = 2\n",
    "min_user_ratings = 20\n",
    "\n",
    "book_counts = rating_df['ISBN'].value_counts()\n",
    "valid_books = book_counts[book_counts > min_book_ratings].index\n",
    "\n",
    "user_counts = rating_df['User-ID'].value_counts()\n",
    "valid_users = user_counts[user_counts > min_user_ratings].index\n",
    "\n",
    "# Remove top 1% of books by rating count\n",
    "top_1_percent = int(len(book_counts) * 0.01)  # Number of books in top 1%\n",
    "top_books = book_counts.head(top_1_percent).index\n",
    "valid_books = valid_books[~valid_books.isin(top_books)]\n",
    "\n",
    "filtered_ratings = rating_df[\n",
    "    (rating_df['ISBN'].isin(valid_books)) &\n",
    "    (rating_df['User-ID'].isin(valid_users)) &\n",
    "    (rating_df['Book-Rating'] != 0)  # Removing zero ratings\n",
    "]\n",
    "\n",
    "df2 = pd.merge(filtered_ratings, books_df, on='ISBN', how='inner')\n",
    "\n",
    "distinct_books = df2['ISBN'].nunique()\n",
    "distinct_users = df2['User-ID'].nunique()\n",
    "print(f\"Number of distinct books: {distinct_books}\")\n",
    "print(f\"Number of distinct users: {distinct_users}\")\n",
    "\n",
    "# Save filtered dataset\n",
    "df2.to_csv('filtered_ratings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a143ef-f7c4-4569-97e5-15ba81f8e135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 10))\n",
    "data_full2 = Dataset.load_from_df(df2[['User-ID', 'ISBN', 'Book-Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04845357-72a5-4d83-a23b-a228c61b5ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "    test_rmse   fit_time  test_time        Algorithm\n",
      "0    1.602884   1.308772   0.361446              SVD\n",
      "1    1.598070  23.744408   4.998938            SVDpp\n",
      "2    1.803182  38.045486   1.677849         SlopeOne\n",
      "3    2.697184   4.566330   0.270253              NMF\n",
      "4    2.458679   0.093774   0.261867  NormalPredictor\n",
      "5    1.702232   0.650849   0.299619      KNNBaseline\n",
      "6    1.928996   0.308214   0.287359         KNNBasic\n",
      "7    1.767089   0.373602   0.281782     KNNWithMeans\n",
      "8    1.770290   0.532455   0.315137    KNNWithZScore\n",
      "9    1.615390   0.395619   0.212578     BaselineOnly\n",
      "10   1.945805   3.575666   0.137397     CoClustering\n"
     ]
    }
   ],
   "source": [
    "benchmark2 = []\n",
    "\n",
    "# Iterate over different algorithms, do cross-validation, and store the RMSE and compare them\n",
    "for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "    results = cross_validate(algorithm, data_full2, measures=['RMSE'], cv=3, verbose=False)\n",
    "\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    algo_name = str(algorithm).split(' ')[0].split('.')[-1]\n",
    "    tmp = pd.concat([tmp, pd.Series({'Algorithm': algo_name})])\n",
    "    benchmark2.append(tmp)\n",
    "\n",
    "benchmark2_df = pd.DataFrame(benchmark2)\n",
    "print(benchmark2_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14a282-e8e1-4447-99d4-b7ed5ff7b079",
   "metadata": {},
   "source": [
    "### **Scenario 2 seems to be giving a slightly better RMSE number**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbabe8-aa72-4bd4-8cbf-94d8b3891d35",
   "metadata": {},
   "source": [
    "### **Now I will do  grid search for 4 individual algorithms SVD, SVDpp, BaselineOnly, and KNNBaseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5dca46f2-8728-4db9-b87b-9ce86fadc389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameter grid for SVD\n",
    "param_grid_svd = {\n",
    "    'n_epochs': [10, 25, 50],\n",
    "    'lr_all': [0.001, 0.005, 0.01],\n",
    "    'reg_all': [0.1, 0.4, 0.08]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV on SVD\n",
    "gs_svd = GridSearchCV(SVD, param_grid_svd, measures=['rmse'], cv=3, n_jobs=-1)\n",
    "gs_svd.fit(data_full2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "953d736a-74fd-4d42-b1fc-b2cfc7000aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5974818683637941\n"
     ]
    }
   ],
   "source": [
    "print(gs_svd.best_score['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44c98fe7-4f0b-451d-9e05-1354c5757909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_epochs': 25, 'lr_all': 0.01, 'reg_all': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(gs_svd.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08a22af9-ca10-42bb-bab5-da7c4f0f752d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameter grid for SVDpp\n",
    "param_grid_svdpp = {'n_epochs': [10, 25, 50], 'lr_all': [0.001, 0.005, 0.01], \n",
    "              'reg_all': [0.1, 0.4, 0.08]}\n",
    "\n",
    "gs_svdpp = GridSearchCV(SVD, param_grid_svdpp, measures = ['rmse'], cv = 3)\n",
    "gs_svdpp.fit(data_full2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e90bd5f4-73bb-43af-ab0c-5ccc025f045f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5927848804736808\n"
     ]
    }
   ],
   "source": [
    "print(gs_svdpp.best_score['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e76591b-4b25-40e5-b3a0-a9b30b04edca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_epochs': 50, 'lr_all': 0.005, 'reg_all': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(gs_svdpp.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616c773-dadb-4180-b662-ee01798e08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch on BaselineOnly\n",
    "param_grid_baselineonly = {'bsl_options': {'method': ['als', 'sgd'],\n",
    "                                  'n_epochs': [10, 25, 50]}\n",
    "              }\n",
    "gs_baselineonly = GridSearchCV(BaselineOnly, param_grid_baselineonly, measures = ['rmse'], cv = 3)\n",
    "gs_baselineonly.fit(data_full2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da27e668-8596-4f66-bd85-21326f0aa360",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.61840953325\n"
     ]
    }
   ],
   "source": [
    "print(gs_baselineonly.best_score['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c700159-ee3b-4eff-9d7f-a31dccfa54a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bsl_options': {'method': 'sgd', 'n_epochs': 25}}\n"
     ]
    }
   ],
   "source": [
    "print(gs_baselineonly.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1da606-4997-4d44-9572-5436762b3bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gridsearch on KNNBaseline\n",
    "param_grid_knnbaseline = {'bsl_options': {'method': ['als']},\n",
    "              'k': [10, 30, 50],\n",
    "              'sim_options': {'name': ['msd', 'cosine', 'pearson', 'pearson_baseline'],\n",
    "                              'min_support': [1, 5],\n",
    "                              'user_based': [True, False]}\n",
    "              }\n",
    "gs_knnbaseline = GridSearchCV(KNNBaseline, param_grid_knnbaseline, measures = ['rmse'], cv = 3)\n",
    "gs_knnbaseline.fit(data_full2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6e8f97f-ff3c-411b-9dcd-0d07fa5d4174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6293849543\n"
     ]
    }
   ],
   "source": [
    "print(gs_knnbaseline.best_score['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08ee7528-3b84-4855-ba9a-1b718fe6a689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bsl_options': {'method': 'als'}, 'k': 10, 'sim_options': {'name': 'pearson_baseline', 'min_support': 5, 'user_based': False}}\n"
     ]
    }
   ],
   "source": [
    "print(gs_knnbaseline.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfd985e-b28d-474b-a095-2f0c0bb7248e",
   "metadata": {},
   "source": [
    "### ** Both SVD and SVDpp seems to be performing the best as compared to other models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec0466b7-e610-4a82-a7a8-daeb52f8649e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_svd_model.joblib']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the best model\n",
    "from joblib import dump\n",
    "\n",
    "# After grid search\n",
    "best_model = gs_svd.best_estimator['rmse']\n",
    "\n",
    "# Save to a pickle file using joblib\n",
    "best_model.fit(data_full2.build_full_trainset())\n",
    "dump(best_model, 'best_svd_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cee28-4fa4-47f2-a9fb-70bf80d5e235",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **using all ratings including 0 and not doing gridsearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba2f62-f039-494e-8e12-18344fc68131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "user_item_matrix = joblib.load('U_I_M_joblib.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6f48de3-6ded-464f-ac5d-9c0fccf00d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replacing null values with 0\n",
    "user_item_matrix = user_item_matrix.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "279529c7-dbdb-421c-8595-0f3792e9c890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since using the Surprise library directly we need to get the data in the required format\n",
    "\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "data = Dataset.load_from_df(user_item_matrix.stack().reset_index(), reader)\n",
    "\n",
    "# doing a 90:10 split\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cc87422-de91-430c-ae5a-a873868ccbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f40c4c49750>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the number of factors for the model\n",
    "n_factors = 150\n",
    "model = SVD(n_factors=n_factors, random_state=42 , n_epochs = 10 , biased = False ,  lr_all=0.01)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5f0f622-1f36-4f97-8cec-9584700addee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.3401\n",
      "RMSE: 0.3401000346136176\n"
     ]
    }
   ],
   "source": [
    "predictions = model.test(testset)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922b6e0-b512-4b90-a7bc-f1995144d357",
   "metadata": {},
   "source": [
    "### **Considering 0 as part of the training definitely underestimates the RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd07dc7a-fe59-461b-bd5e-5d3dc761d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 3641\n",
      "Number of items: 2894\n",
      "Number of ratings: 16918\n",
      "Total possible ratings: 10537054\n",
      "📉 Sparsity of the user-item matrix: 0.9984 (99.84%)\n"
     ]
    }
   ],
   "source": [
    "# How much sparsity still remains in the data\n",
    "import pandas as pd\n",
    "\n",
    "# Load your processed ratings file if not already loaded\n",
    "rating_df = pd.read_csv('filtered_ratings.csv')\n",
    "\n",
    "# Exclude implicit (0) ratings if needed\n",
    "rating_df_explicit = rating_df[rating_df['Book-Rating'] > 0]\n",
    "\n",
    "# Unique users and items\n",
    "num_users = rating_df_explicit['User-ID'].nunique()\n",
    "num_items = rating_df_explicit['ISBN'].nunique()\n",
    "\n",
    "# Number of ratings\n",
    "num_ratings = len(rating_df_explicit)\n",
    "\n",
    "# Total possible interactions\n",
    "total_possible = num_users * num_items\n",
    "\n",
    "# Sparsity calculation\n",
    "sparsity = 1 - (num_ratings / total_possible)\n",
    "\n",
    "print(f\"Number of users: {num_users}\")\n",
    "print(f\"Number of items: {num_items}\")\n",
    "print(f\"Number of ratings: {num_ratings}\")\n",
    "print(f\"Total possible ratings: {total_possible}\")\n",
    "print(f\"📉 Sparsity of the user-item matrix: {sparsity:.4f} ({sparsity * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315e0f5-09db-4d35-a1d3-eedac421b9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
